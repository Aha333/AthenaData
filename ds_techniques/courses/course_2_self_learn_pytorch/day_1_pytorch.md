Day 1

# pytorch 里面的model可以简单的理解成一个函数， 需要去fit。 

# Tensor 和 .grad 属性
普通 Tensor：不是所有的 Tensor 都会有 .grad 属性。只有需要计算梯度的 Tensor（即参与反向传播计算的 Tensor）才会有 .grad 属性。并且，只有那些**由 requires_grad=True**的 Tensor 才会计算和存储梯度。
requires_grad=True：这是 PyTorch 中一个非常重要的属性。只有设置了 requires_grad=True 的 Tensor 才会记录梯度，并且它的 .grad 属性才能存储计算出的梯度。

# backward() 通常只能在标量张量上直接调用。


# requires_grad 数学含义是不是说明这个是一个偏微分的求导variable
是的，requires_grad=True 的数学含义可以理解为：这个张量（tensor）是参与求导过程的变量，也就是说，它会记录和计算关于它的偏导数（梯度）。在反向传播中，PyTorch 会计算那些 requires_grad=True 的张量的梯度，并将其存储在 .grad 属性中。

# pytorch tensor model 优化 原理
这里的原理是，tensor 的计算过程会通过计算图追溯，这就是为什么可以在任何时候自动计算梯度的原因。第二，model 本身是一个计算表达式，它定义了输入与输出之间的关系，但最终输出的是预测值（即标签的预测）。第三，**损失函数（loss）**才是我们真正优化的目标，它是由多个张量的复杂组合构成的，并且这些张量之间通过计算图建立了关系。当调用 backward() 时，PyTorch 会自动根据计算图计算梯度（偏导数）。

# 激活函数比如要有吗
激活函数在神经网络中是非常关键的，通常必须有。
假设我们有一个简单的神经网络，它有两层：输入层和输出层。如果两层都没有激活函数，网络的输出其实只是输入和权重的线性组合。
还是线性的。

但如果我们在每一层后面加上非线性激活函数，就可以使得每一层不仅仅是简单的加权和，变成了一个更复杂的非线性映射，从而大大增强了模型的表示能力。
激活函数的核心作用是引入非线性。

#为什么神经网络中常用的激活函数是 ReLU、Sigmoid 和 Tanh，其他非线性函数为什么不常用？
TL;DR：
ReLU、Sigmoid 和 Tanh 被广泛使用是因为它们能有效解决梯度消失问题（特别是 ReLU）并且计算效率高。ReLU 系列（尤其是 ReLU 和 Leaky ReLU）在深度网络中表现出色，因为它们能加速训练，避免死神经元问题，而 Sigmoid 和 Tanh 则主要用于输出层（如二分类问题）。


# 是不是所有的非线性激活函数都可以逼近任何连续函数？
是的，通用逼近定理表明，所有适当的非线性激活函数（如 Sigmoid、Tanh、ReLU 等）理论上都能逼近任何连续函数，前提是网络有足够的神经元和层数。

通用逼近定理的核心：

该定理表明，只要激活函数是非线性的，并且神经网络有足够的神经元和层数，那么神经网络就能够逼近任何目标函数。这个目标函数必须是连续的。

换句话说，神经网络（特别是前馈神经网络）能够在理论上逼近任何连续的函数，无论这个函数有多复杂。

# 除了前馈神经网络（Feedforward Neural Network, FNN）之外，还有哪些其他类型的神经网络？
除了前馈神经网络外，还有很多类型的神经网络，每种神经网络结构都有其特定的应用场景，包括卷积神经网络（CNN）、循环神经网络（RNN）、生成对抗网络（GAN）等。


# 常见的机器学习模型，如 XGBoost 和线性回归，能退化为哪一种类型的神经网络？
线性回归模型可以被视作一个非常简单的 前馈神经网络（FNN）； 一个包含一个输入层和一个输出层的神经网络，其中没有激活函数。实际上，这个模型相当于一个没有激活函数的 单层前馈神经网络。

XGBoost 使用的 决策树 可以被看作是一种具有非线性的 模型单元; 
本身不能直接退化为神经网络，因为它是基于树的模型。
不过，从训练的角度来看，XGBoost 和神经网络的 梯度下降优化 和 损失函数最小化 有相似之处，
但 XGBoost 使用的 基学习器是决策树，而神经网络使用的 基学习器是神经元节点；
# 神经元节点是什么？它是一个变量吗？
神经元节点（或简称神经元）是神经网络中的一个基本计算单元。每个神经元接收输入，进行加权和偏置操作，然后通过激活函数输出结果。它不仅仅是一个变量，而是神经网络中的一个功能单元，通常包含权重、偏置以及激活函数。


# 训练神经网络的流程
现在我们懂了这个术语了 
训练神经网络的流程
训练神经网络的基本流程如下：

前向传播：通过神经网络计算输出，得到预测结果。

计算损失：根据模型输出与真实标签计算损失函数。

反向传播：计算损失函数关于模型参数的梯度（即偏导数）。

更新参数：使用优化算法（如梯度下降、Adam）更新模型的参数。

重复步骤1-4，直到损失收敛或达到预定的训练轮数。


# DataLoader -》 batch load
from torch.utils.data import DataLoader
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

DataLoader 的作用是什么？
在 PyTorch 中，DataLoader 是一个非常重要的类，它的作用是帮助我们以批量（batch）的方式加载数据。通常来说，深度学习模型训练时，数据量非常大，我们不可能一次性将所有数据都加载到内存中。因此，DataLoader 会将数据分成多个小批量（batch），然后在每次训练时按批次加载数据。

# Epoch 的含义 - 完整的遍历一次
在机器学习（尤其是深度学习）中，Epoch 是指 训练过程中数据集的完整遍历。简单来说，一个Epoch 就是训练数据集中的每一个样本都被模型处理和学习一次。

## 与 Batch 的关系：

在深度学习训练中，数据集通常被分成多个 batch。每个 batch 会包含多个样本。
一个 Epoch 就是 整个数据集 都被用于训练一次，数据集被分成多个小批次（batch），模型依次处理每个批次，直到处理完整个数据集。

## xgb 类似于 Epoch 的概念是什么
-Boosting 迭代次数（n_estimators）
在 XGBoost 中，通常使用 n_estimators 来指定训练过程中 迭代的次数。每一次迭代都等于训练一棵新树，因此可以认为每个迭代步（boosting iteration）类似于一个 Epoch。

https://github.com/Aha333/Dive-into-DL-PyTorch

# 优化器
# 优化器是否可以被理解为一个控制模型参数如何更新的程序？
是的，优化器在深度学习中控制模型参数的更新方式，它决定了如何根据损失函数的梯度来调整模型的参数，从而帮助模型优化。
优化器不仅仅是一个“更新”过程，它还包含了如何根据梯度做出决策，以及如何在每次迭代中选择步长（学习率）等。

一些优化器（如 SGD 和 Adam）还有其他重要的超参数，比如 动量（momentum）、权重衰减（weight decay） 等。

动量（Momentum）：可以帮助优化器加速收敛，尤其在有噪声或震荡的梯度中。动量会保留之前的梯度信息，使得更新不仅仅依赖当前梯度。 权重衰减（Weight Decay）：防止模型过拟合，通常通过 L2 正则化 来实现。


# 模型评估的关键点：
训练模式 vs 评估模式：在训练过程中，神经网络会使用 dropout 和 batch normalization 等技术，这些技术在训练和评估时的行为是不同的。model.train() 和 model.eval() 用来切换训练和评估模式。

不计算梯度：在评估时，torch.no_grad() 被用来禁止计算梯度，这样可以节省内存，并加快验证过程。

# MLP
MLP 是前馈神经网络的一个变种，它有多个隐藏层，而前馈神经网络（FFNN）可能只有一个隐藏层或甚至没有隐藏层。

普通的 FFNN 更多的是指网络结构相对简单的模型，只有少量的层次。

MLP 通常指的是更复杂的前馈神经网络，具有多个隐藏层以及非线性激活函数，可以处理更复杂的任务。

# Encoder-Decoder 与 MLP 的关联和区别是什么
Encoder-Decoder 是一种用于处理 序列到序列 问题的神经网络架构，特别适合处理变长输入和输出的任务（如机器翻译）。而 MLP（多层感知机） 是一种传统的前馈神经网络架构，适用于 固定长度的输入和输出 问题（如分类、回归）。两者的主要区别在于输入和输出的结构，Encoder-Decoder 处理序列数据，MLP 处理固定维度的数据。

MLP： 手写数字识别（例如 MNIST）、图像分类（例如 CIFAR-10）等任务。

架构：Encoder-Decoder 是一种 序列到序列（Seq2Seq） 的模型架构： Encoder（编码器）：负责将输入序列编码成一个固定的向量（上下文向量），以压缩输入的信息。Decoder（解码器）：根据上下文向量生成输出序列，通常是逐步生成每个输出的过程。
机器翻译（如英文到法文的翻译）、语音识别（如音频到文本的转换）等


# 常见的神经网络类型有：

卷积神经网络（CNN）：用于图像和视觉任务。

递归神经网络（RNN）：用于处理时序数据。

长短时记忆网络（LSTM）：改进的 RNN，适用于长时间依赖任务。

生成对抗网络（GAN）：用于生成模型，通过对抗训练生成数据。

自编码器（Autoencoder）：无监督学习，用于数据压缩、降噪等。

Transformer 网络：基于注意力机制，广泛用于自然语言处理。

图神经网络（GNN）：用于图结构数据。

强化学习网络：用于智能体学习与环境交互。

注意力机制：允许模型集中注意力处理输入序列的关键部分。



# LSTM的创新点 （还是不懂）
LSTM的结构可以看作是对传统RNN的改进，它引入了“细胞状态”和“门控机制”，让网络能够有选择地记住和忘记信息，从而解决了传统RNN在长序列数据处理中的问题。具体来说：

细胞状态：保持着网络的“长期记忆”，不容易丢失。

门控机制：通过遗忘门、输入门和输出门来控制信息的流动，允许LSTM选择性地记住或遗忘信息。

通过这种创新的设计，LSTM能够有效地处理长期依赖问题，解决了传统RNN的梯度消失问题。


# Transformer 的
Transformer 是由 Vaswani 等人在 2017 年提出的模型，旨在解决传统的 RNN 和 LSTM 在处理长序列时的效率和性能问题。Transformer 采用了一种新的机制——自注意力机制（Self-Attention），并摒弃了 RNN 中的递归结构，改用了完全基于注意力机制的结构。


# 理解softmax 归一化操作
softmax(⋅) 是一个归一化操作，将每个查询的结果转换为概率分布，确保权重之和为 1。