{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be55713a-0de2-4157-8c36-18712f2bcd9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn\n",
      "File \u001b[0;32m~/Documents/AthenaData/venv/lib/python3.11/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/AthenaData/venv/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:362\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    360\u001b[0m             warn(msg)\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_from_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimezone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m~/Documents/AthenaData/venv/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:511\u001b[0m, in \u001b[0;36mSparkConversionMixin._convert_from_pandas\u001b[0;34m(self, pdf, schema, timezone)\u001b[0m\n\u001b[1;32m    504\u001b[0m             pdf[column] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\n\u001b[1;32m    505\u001b[0m                 ser\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mto_pytimedelta(), index\u001b[38;5;241m=\u001b[39mser\u001b[38;5;241m.\u001b[39mindex, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mser\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    506\u001b[0m             )\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Convert pandas.DataFrame to list of numpy records\u001b[39;00m\n\u001b[1;32m    509\u001b[0m np_records \u001b[38;5;241m=\u001b[39m \u001b[43mpdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m--> 511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# Check if any columns need to be fixed for Spark to infer properly\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np_records) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/AthenaData/venv/lib/python3.11/site-packages/pandas/core/frame.py:2479\u001b[0m, in \u001b[0;36mDataFrame.to_records\u001b[0;34m(self, index, column_dtypes, index_dtypes)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype_mapping\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m specified for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2477\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 2479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformats\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AthenaData/venv/lib/python3.11/site-packages/numpy/core/records.py:643\u001b[0m, in \u001b[0;36mfromarrays\u001b[0;34m(arrayList, dtype, shape, formats, names, titles, aligned, byteorder)\u001b[0m\n\u001b[1;32m    640\u001b[0m shape \u001b[38;5;241m=\u001b[39m _deprecate_shape_0_as_None(shape)\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 643\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[43marrayList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    645\u001b[0m     shape \u001b[38;5;241m=\u001b[39m (shape,)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "spark.createDataFrame(pd.DataFrame()).withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ebd23b-9b92-4f69-9fe3-2dd6171e36ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `F.sum` not found.\n"
     ]
    }
   ],
   "source": [
    "?F.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd3478aa-cc99-425b-9ba3-9f9928b1b16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Testing with 1000 rows ====================\n",
      "=== BEGINNER LEVEL EXAMPLES ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/16 11:37:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering Performance:\n",
      "Pandas: 0.00s\n",
      "Spark: 1.43s\n",
      "\n",
      "Column Selection Performance:\n",
      "Pandas: 0.00s\n",
      "Spark: 0.16s\n",
      "\n",
      "Basic Aggregation Performance:\n",
      "Pandas: 0.00s\n",
      "Spark: 0.46s\n",
      "\n",
      "=== INTERMEDIATE LEVEL EXAMPLES ===\n",
      "\n",
      "Window Functions Performance:\n",
      "Pandas: 0.00s\n",
      "Spark: 0.33s\n",
      "\n",
      "Joins Performance:\n",
      "Pandas: 0.00s\n",
      "Spark: 0.34s\n",
      "\n",
      "=== ADVANCED LEVEL EXAMPLES ===\n",
      "\n",
      "Custom Functions Performance:\n",
      "Pandas: 0.00s\n",
      "Spark: 0.53s\n",
      "\n",
      "Complex Transformations Performance:\n",
      "Pandas: 0.00s\n",
      "Spark: 0.57s\n",
      "\n",
      "==================== Testing with 100000 rows ====================\n",
      "=== BEGINNER LEVEL EXAMPLES ===\n",
      "\n",
      "Filtering Performance:\n",
      "Pandas: 0.00s\n",
      "Spark: 0.56s\n",
      "\n",
      "Column Selection Performance:\n",
      "Pandas: 0.00s\n",
      "Spark: 0.32s\n",
      "\n",
      "Basic Aggregation Performance:\n",
      "Pandas: 0.00s\n",
      "Spark: 0.20s\n",
      "\n",
      "=== INTERMEDIATE LEVEL EXAMPLES ===\n",
      "\n",
      "Window Functions Performance:\n",
      "Pandas: 0.02s\n",
      "Spark: 0.86s\n",
      "\n",
      "Joins Performance:\n",
      "Pandas: 0.00s\n",
      "Spark: 0.33s\n",
      "\n",
      "=== ADVANCED LEVEL EXAMPLES ===\n",
      "\n",
      "Custom Functions Performance:\n",
      "Pandas: 0.22s\n",
      "Spark: 0.58s\n",
      "\n",
      "Complex Transformations Performance:\n",
      "Pandas: 0.02s\n",
      "Spark: 0.89s\n",
      "\n",
      "==================== Testing with 1000000 rows ====================\n",
      "=== BEGINNER LEVEL EXAMPLES ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/16 11:37:38 WARN TaskSetManager: Stage 54 contains a task of very large size (2115 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering Performance:\n",
      "Pandas: 0.02s\n",
      "Spark: 1.99s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/16 11:37:40 WARN TaskSetManager: Stage 55 contains a task of very large size (2115 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Selection Performance:\n",
      "Pandas: 0.01s\n",
      "Spark: 1.77s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/16 11:37:42 WARN TaskSetManager: Stage 56 contains a task of very large size (2115 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic Aggregation Performance:\n",
      "Pandas: 0.03s\n",
      "Spark: 0.38s\n",
      "\n",
      "=== INTERMEDIATE LEVEL EXAMPLES ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/16 11:37:54 WARN TaskSetManager: Stage 59 contains a task of very large size (2115 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Window Functions Performance:\n",
      "Pandas: 0.29s\n",
      "Spark: 4.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/16 11:37:58 WARN TaskSetManager: Stage 65 contains a task of very large size (2115 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Joins Performance:\n",
      "Pandas: 0.03s\n",
      "Spark: 0.48s\n",
      "\n",
      "=== ADVANCED LEVEL EXAMPLES ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/16 11:38:11 WARN TaskSetManager: Stage 71 contains a task of very large size (2115 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Functions Performance:\n",
      "Pandas: 2.27s\n",
      "Spark: 3.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/16 11:38:25 WARN TaskSetManager: Stage 72 contains a task of very large size (2115 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/02/16 11:38:26 WARN TaskSetManager: Stage 75 contains a task of very large size (2115 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 75:=====>                                                   (1 + 9) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complex Transformations Performance:\n",
      "Pandas: 0.17s\n",
      "Spark: 2.22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------                                        \n",
      "Exception occurred during processing of request from ('127.0.0.1', 54104)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/daweideng/Documents/AthenaData/venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/daweideng/Documents/AthenaData/venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/Users/daweideng/Documents/AthenaData/venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/daweideng/Documents/AthenaData/venv/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "def create_spark_session(app_name: str = \"PySpark Practice\") -> SparkSession:\n",
    "    \"\"\"Create or get a Spark Session\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def generate_sample_data(n_rows: int = 1000000) -> pd.DataFrame:\n",
    "    \"\"\"Generate sample data for practice\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'user_id': range(n_rows),\n",
    "        'age': np.random.randint(18, 80, n_rows),\n",
    "        'gender': np.random.choice(['M', 'F'], n_rows),\n",
    "        'purchase_amount': np.random.normal(100, 30, n_rows),\n",
    "        'category': np.random.choice(['A', 'B', 'C', 'D'], n_rows),\n",
    "    })\n",
    "\n",
    "def compare_performance(\n",
    "    pandas_func, \n",
    "    spark_func, \n",
    "    pandas_df: pd.DataFrame, \n",
    "    spark_df = None\n",
    ") -> Tuple[float, float, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compare performance between Pandas and PySpark operations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pandas_func : callable\n",
    "        Function that operates on pandas DataFrame\n",
    "    spark_func : callable\n",
    "        Function that operates on spark DataFrame\n",
    "    pandas_df : pd.DataFrame\n",
    "        Input pandas DataFrame\n",
    "    spark_df : pyspark.sql.DataFrame, optional\n",
    "        Input spark DataFrame. If None, will be created from pandas_df\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[float, float, pd.DataFrame, pd.DataFrame] : \n",
    "        (pandas_time, spark_time, pandas_result, spark_result)\n",
    "    \"\"\"\n",
    "    # Pandas timing\n",
    "    start = time.time()\n",
    "    pandas_result = pandas_func(pandas_df)\n",
    "    pandas_time = time.time() - start\n",
    "    \n",
    "    # Get or create Spark DataFrame\n",
    "    if spark_df is None:\n",
    "        spark = create_spark_session()\n",
    "        spark_df = spark.createDataFrame(pandas_df)\n",
    "    \n",
    "    # PySpark timing\n",
    "    start = time.time()\n",
    "    spark_result = spark_func(spark_df)\n",
    "    # Convert to Pandas for comparison (if it's not already collected)\n",
    "    if hasattr(spark_result, 'toPandas'):\n",
    "        spark_result = spark_result.toPandas()\n",
    "    spark_time = time.time() - start\n",
    "    \n",
    "    return pandas_time, spark_time, pandas_result, spark_result\n",
    "\n",
    "# ===================== BEGINNER LEVEL =====================\n",
    "\n",
    "def basic_operations_example(n_rows: int = 1000000):\n",
    "    \"\"\"\n",
    "    Demonstrate basic operations in both Pandas and PySpark:\n",
    "    - DataFrame creation\n",
    "    - Basic filtering\n",
    "    - Column selection\n",
    "    - Simple aggregation\n",
    "    \"\"\"\n",
    "    print(\"=== BEGINNER LEVEL EXAMPLES ===\")\n",
    "    \n",
    "    # Generate sample data\n",
    "    pdf = generate_sample_data(n_rows)\n",
    "    spark = create_spark_session()\n",
    "    sdf = spark.createDataFrame(pdf)\n",
    "    \n",
    "    # 1. Basic Filtering\n",
    "    def pandas_filter(df):\n",
    "        return df[df['age'] > 30]\n",
    "    \n",
    "    def spark_filter(df):\n",
    "        return df.filter(F.col('age') > 30)\n",
    "    \n",
    "    p_time, s_time, p_result, s_result = compare_performance(pandas_filter, spark_filter, pdf, sdf)\n",
    "    print(f\"\\nFiltering Performance:\")\n",
    "    print(f\"Pandas: {p_time:.2f}s\")\n",
    "    print(f\"Spark: {s_time:.2f}s\")\n",
    "    \n",
    "    # 2. Column Selection\n",
    "    def pandas_select(df):\n",
    "        return df[['user_id', 'age', 'purchase_amount']]\n",
    "    \n",
    "    def spark_select(df):\n",
    "        return df.select('user_id', 'age', 'purchase_amount')\n",
    "    \n",
    "    p_time, s_time, p_result, s_result = compare_performance(pandas_select, spark_select, pdf, sdf)\n",
    "    print(f\"\\nColumn Selection Performance:\")\n",
    "    print(f\"Pandas: {p_time:.2f}s\")\n",
    "    print(f\"Spark: {s_time:.2f}s\")\n",
    "    \n",
    "    # 3. Basic Aggregation\n",
    "    def pandas_agg(df):\n",
    "        return df.groupby('category')['purchase_amount'].mean()\n",
    "    \n",
    "    def spark_agg(df):\n",
    "        return df.groupBy('category').agg(F.mean('purchase_amount'))\n",
    "    \n",
    "    p_time, s_time, p_result, s_result = compare_performance(pandas_agg, spark_agg, pdf, sdf)\n",
    "    print(f\"\\nBasic Aggregation Performance:\")\n",
    "    print(f\"Pandas: {p_time:.2f}s\")\n",
    "    print(f\"Spark: {s_time:.2f}s\")\n",
    "\n",
    "# ===================== INTERMEDIATE LEVEL =====================\n",
    "\n",
    "def intermediate_operations_example(n_rows: int = 1000000):\n",
    "    \"\"\"\n",
    "    Demonstrate intermediate operations:\n",
    "    - Window functions\n",
    "    - Complex aggregations\n",
    "    - Joins\n",
    "    \"\"\"\n",
    "    print(\"\\n=== INTERMEDIATE LEVEL EXAMPLES ===\")\n",
    "    \n",
    "    # Generate main data\n",
    "    pdf1 = generate_sample_data(n_rows)\n",
    "    spark = create_spark_session()\n",
    "    sdf1 = spark.createDataFrame(pdf1)\n",
    "    \n",
    "    # Generate additional data for joins\n",
    "    pdf2 = pd.DataFrame({\n",
    "        'category': ['A', 'B', 'C', 'D'],\n",
    "        'category_name': ['Electronics', 'Clothing', 'Food', 'Books'],\n",
    "        'discount_rate': [0.1, 0.2, 0.15, 0.25]\n",
    "    })\n",
    "    sdf2 = spark.createDataFrame(pdf2)\n",
    "    \n",
    "    # 1. Window Functions\n",
    "    def pandas_window(df):\n",
    "        return df.assign(\n",
    "            avg_by_gender=df.groupby('gender')['purchase_amount'].transform('mean'),\n",
    "            rank_by_amount=df.groupby('category')['purchase_amount'].rank(method='dense')\n",
    "        )\n",
    "    \n",
    "    def spark_window(df):\n",
    "        window_gender = Window.partitionBy('gender')\n",
    "        window_category = Window.partitionBy('category').orderBy(F.desc('purchase_amount'))\n",
    "        \n",
    "        return df.select(\n",
    "            '*',\n",
    "            F.avg('purchase_amount').over(window_gender).alias('avg_by_gender'),\n",
    "            F.dense_rank().over(window_category).alias('rank_by_amount')\n",
    "        )\n",
    "    \n",
    "    p_time, s_time, p_result, s_result = compare_performance(\n",
    "        pandas_window, spark_window, pdf1, sdf1\n",
    "    )\n",
    "    print(f\"\\nWindow Functions Performance:\")\n",
    "    print(f\"Pandas: {p_time:.2f}s\")\n",
    "    print(f\"Spark: {s_time:.2f}s\")\n",
    "    \n",
    "    # 2. Joins with aggregation\n",
    "    def pandas_join(df):\n",
    "        agg_df = df.groupby('category')['purchase_amount'].agg(['mean', 'count']).reset_index()\n",
    "        return agg_df.merge(pdf2, on='category', how='left')\n",
    "    \n",
    "    def spark_join(df):\n",
    "        agg_df = df.groupBy('category').agg(\n",
    "            F.avg('purchase_amount').alias('mean'),\n",
    "            F.count('purchase_amount').alias('count')\n",
    "        )\n",
    "        return agg_df.join(sdf2, on='category', how='left')\n",
    "    \n",
    "    p_time, s_time, p_result, s_result = compare_performance(\n",
    "        pandas_join, spark_join, pdf1, sdf1\n",
    "    )\n",
    "    print(f\"\\nJoins Performance:\")\n",
    "    print(f\"Pandas: {p_time:.2f}s\")\n",
    "    print(f\"Spark: {s_time:.2f}s\")\n",
    "\n",
    "# ===================== ADVANCED LEVEL =====================\n",
    "\n",
    "def advanced_operations_example(n_rows: int = 1000000):\n",
    "    \"\"\"\n",
    "    Demonstrate advanced operations:\n",
    "    - Custom UDFs\n",
    "    - Complex transformations\n",
    "    - Performance optimization techniques\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ADVANCED LEVEL EXAMPLES ===\")\n",
    "    \n",
    "    pdf = generate_sample_data(n_rows)\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    # 1. Custom Functions/UDFs\n",
    "    # Pandas: Simple function application\n",
    "    def calculate_discount(amount, age):\n",
    "        return amount * (1 - (age/100))\n",
    "    \n",
    "    def pandas_udf(df):\n",
    "        return df.assign(\n",
    "            discounted_amount=df.apply(\n",
    "                lambda x: calculate_discount(x['purchase_amount'], x['age']), \n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Spark: Registered UDF\n",
    "    spark_calculate_discount = F.udf(\n",
    "        lambda amount, age: float(amount * (1 - (age/100))),\n",
    "        FloatType()\n",
    "    )\n",
    "    \n",
    "    def spark_udf(df):\n",
    "        return df.withColumn(\n",
    "            'discounted_amount',\n",
    "            spark_calculate_discount('purchase_amount', 'age')\n",
    "        )\n",
    "    \n",
    "    p_time, s_time, p_result, s_result = compare_performance(pandas_udf, spark_udf, pdf, None)\n",
    "    print(f\"\\nCustom Functions Performance:\")\n",
    "    print(f\"Pandas: {p_time:.2f}s\")\n",
    "    print(f\"Spark: {s_time:.2f}s\")\n",
    "    \n",
    "    # 2. Complex Transformations\n",
    "    def pandas_complex(df):\n",
    "        return df.assign(\n",
    "            age_group=pd.qcut(df['age'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "        ).groupby(['age_group', 'gender'])\\\n",
    "          .agg({\n",
    "              'purchase_amount': ['mean', 'std', 'count'],\n",
    "              'user_id': 'nunique'\n",
    "          }).reset_index()\n",
    "    \n",
    "    def spark_complex(df):\n",
    "        # First calculate the percentiles\n",
    "        percentiles = df.select(\n",
    "            F.percentile_approx('age', [0.25, 0.5, 0.75]).alias('percentiles')\n",
    "        ).collect()[0]['percentiles']\n",
    "        \n",
    "        # Then use these values to create age groups\n",
    "        df_with_groups = df.withColumn(\n",
    "            'age_group',\n",
    "            F.when(F.col('age') <= percentiles[0], 'Q1')\n",
    "             .when(F.col('age') <= percentiles[1], 'Q2')\n",
    "             .when(F.col('age') <= percentiles[2], 'Q3')\n",
    "             .otherwise('Q4')\n",
    "        )\n",
    "        \n",
    "        # Finally perform the groupBy aggregations\n",
    "        return df_with_groups.groupBy('age_group', 'gender').agg(\n",
    "            F.mean('purchase_amount').alias('purchase_mean'),\n",
    "            F.stddev('purchase_amount').alias('purchase_std'),\n",
    "            F.count('purchase_amount').alias('purchase_count'),\n",
    "            F.countDistinct('user_id').alias('unique_users')\n",
    "        )\n",
    "    \n",
    "    p_time, s_time, p_result, s_result = compare_performance(pandas_complex, spark_complex, pdf, None)\n",
    "    print(f\"\\nComplex Transformations Performance:\")\n",
    "    print(f\"Pandas: {p_time:.2f}s\")\n",
    "    print(f\"Spark: {s_time:.2f}s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run examples with different data sizes\n",
    "    for n_rows in [1000, 100000, 1000000]:\n",
    "        print(f\"\\n{'='*20} Testing with {n_rows} rows {'='*20}\")\n",
    "        basic_operations_example(n_rows)\n",
    "        intermediate_operations_example(n_rows)\n",
    "        advanced_operations_example(n_rows) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915cfadb-9634-464a-9bc1-587de757eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "#print(spark.conf.get(\"spark.executor.cores\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f680262-b2cf-4142-80a4-1bb42f4cb1f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m percentiles \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m      2\u001b[0m             F\u001b[38;5;241m.\u001b[39mpercentile_approx(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.75\u001b[39m])\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercentiles\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "pdf = generate_sample_data(n_rows)\n",
    "percentiles = df.select(\n",
    "            F.percentile_approx('age', [0.25, 0.5, 0.75]).alias('percentiles')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801ab8a0-bd56-4367-b3dd-4e52fb0ba4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f41b6aa-1172-41ed-b082-67e468968dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local[*]\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.master\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b554238-8705-4e11-96bf-06c1596a6022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_athenadata",
   "language": "python",
   "name": "venv_athenadata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
